{% extends "blog_frame.html" %}
{% block scripts %}
<link href="/static/css/prism.css" rel="stylesheet" />
<script src="/static/js/prism.js"></script>
{% endblock %}
{% block content %}

<p>
    One of the most important realizations of working with information is that data never comes neatly organized. Real
    world data is, merely by its numerical nature, invariably messy, requiring significant clean-up (and oftentimes
    domain expertise) to render usable. This is <strong>data noise</strong>.
</p>
<!--<p>-->
    <!--For example, suppose a dataset extracted from a SQL table containing a column of addresses stored in a-->
    <!--fixed-width 20-character SQL table. Then for example the address <code class="inline_code">5 AVENUE OF THE-->
    <!--AMERICAS</code> would be cut off as <code class="inline_code">5 AVENUE OF THE AMER</code>, a partial data which-->
    <!--could only be converted to an actual address either by hand or through the additional application (and work) of-->
    <!--invoking a <em>very</em> good <a href="https://en.wikipedia.org/wiki/Geocoding">geocoder</a>.-->
<!--</p>-->
<!--<p>-->
    <!--This isn't a made-up example—this is exactly what New York City's <a-->
        <!--href="http://www1.nyc.gov/site/planning/data-maps/open-data/dwn-pluto-mappluto.page">PLUTO</a>, a record of-->
    <!--all buildings in New York City, does.-->
<!--</p>-->
<p>
    Almost all datasets contain some noise, and the less noise there is, the better ("cleaner") the dataset. Some
    kinds of noise are easier to correct than others; sometimes removing the noise completely is simply impossible
    and all you can do is resample or hope for the best, keeping in mind that <a
        href="https://en.wikipedia.org/wiki/Garbage_in,_garbage_out">when garbage goes in, garbage comes out</a>.
</p>
<p>
    This post explores a form of both noise and not-noise that's often discussed but rarely quantified, and critical
    to understand: <strong>null data</strong>. We'll dive into nullity semantics, look at why these representations
    are useful, discuss the technical implementation and limitations of nullity in Python, and look at a few
    practical examples.
</p>

<h3>So what is null data?</h3>

<p>
    What is null data? Let's take an illustrative example. Suppose you had the following data (a sample of
    columns from the <a
        href="https://data.cityofnewyork.us/Public-Safety/NYPD-Motor-Vehicle-Collisions/h9gi-nx95">NYPD Motor
    Vehicle Collision Dataset</a>):
</p>

<img src="/static/post_assets/null-and-missing-data-python/table_1.png" class="inline_image_full" alt="Data."/>

<p>
    Each of these records corresponds with an accident reported and tended to by the New York City Police Department
    (NYPD) with some concrete location. Yet as this selection shows, some records are <em>completely
</em> missing this data!
</p>
<p>
    This is an example of <strong>missing data</strong>—data that we know exists, but which, due to sparse or
    incomplete data collection, we do not actually know the value of.
</p>
<p>
    Here's another example (from <a
        href="http://www.nytimes.com/2013/07/22/business/in-climbing-income-ladder-location-matters.html?_r=0">an
    excellent <em>New York Times</em> article</a>):
</p>

<img src="/static/post_assets/null-and-missing-data-python/nyt-income-mobility-example.png"
     class="inline_image_full" alt="Data."/>

<p>
    Notice that there are certain counties in this map—one in Georgia, one in Texas, and so on—which lack data. This
    isn't because there isn't income inequality in these areas. Rather there is some sort of data there that defied
    measurement, making these counties once again instances of <strong>missing data</strong>.
</p>

<p>
    But not all data is merely incomplete. Sometimes it really doesn't exist. Let's return to our earlier NYPD dataset:
</p>

<img src="/static/post_assets/null-and-missing-data-python/table_2.png" class="inline_image_full" alt="Data."/>

<p>
    A collision location is recorded as a combination of <code class="inline_code">ON STREET</code> and <code
        class="inline_code">CROSS STREET</code> in incidents in which cars collide mid-street or mid-avenue. But not
    all collisions occur on streets; some occur <code class="inline_code">OFF STREET</code>, like, say, in parking
    lots.
</p>
<!--<p>-->
    <!--How do we address this clearly? By, as in this dataset, creating three columns and coding in either only the-->
    <!--first two or only the last one, as appropriate.-->
<!--</p>-->
<p>
    Then the other columns don't carry any informational content at all. They exemplify <strong>non-data</strong>.
</p>
<p>
    A second illustrative example—this time a map of all single-family home property
    sales in New York City:
</p>

<img src="/static/post_assets/null-and-missing-data-python/single-family-home-sales.png" class="inline_image_full" alt="Data."/>

<p>
    Notice that there are certain areas, like Central Park, which are grayed out. These public areas simply do not
    contain residential property sales at all. You can't divide by zero—these areas are again examples of
    <strong>non-data</strong>.
</p>
<p>
    This means that when answering the question "is this data entry filled?" one must actually consider three
    possible answers: "Yes", "No, but it can be", and "No, and it cannot be".
</p>

<h3>Why does null data matter?</h3>


<p>
    Non-data isn't erroneous—it's existentially undefined, part of the structure of the data you are working
    with. Missing data, meanwhile, is a question of actionability: is this data noise something that I, the
    programmer, can reasonably attempt to correct?
</p>
<p>
    Statisticians distinguish between two types of missing data.
</p>
<p>
    <strong>Data missing at random</strong> is a nullity pattern which occurs randomly. For example, in the motor
    vehicle collision dataset above we may reasonably assume that observations missing collision streetnames are
    randomly distributed throughout the police precincts.
</p>
<p> In the simple case of having a sufficiently large number of observations for such data to be statistically
    irrelevant, you can safely drop these observations using e.g. <a
            href="http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html"><code
            class="inline_code">pd.DataFrame.drop()</code></a>.
</p>
<p>
    Far more troubling is <strong>data not missing at random</strong>. For example, for the motor vehicle collision
    dataset I discovered that NYPD officers merely provide streetnames in their collision reports—the <code>LOCATION
</code> column is geocoded from there. Since <code class="inline_code">ROOSEVELT AVENUE AND 83 STREET</code> is far
    easier to geocode than <code class="inline_code">PARKING LOT OF 79-15 MAIN STREET</code> is, off-street
    collisions are missing coordinate data at a higher rate than on-street collisions, a difference in nullity
    pattern which absolutely must be taken into account in interpreting the dataset as a whole.
</p>
<p>
    And when you know or have reason to believe that data is not missing at random, dropping the data and modeling
    what's left would result in falsehoods, ignoring a potentially important determinant variable or variables
    which, for convenience's sake, you have simply deleted. In these cases interpolation techniques become a
    necessity—<code class="inline_code">pandas</code> has a <a
        href="http://pandas.pydata.org/pandas-docs/stable/missing_data.html">whole tutorial</a> on the subject.
</p>
<p>
    There's a lot of definitions floating around at this point, so here's a sketch of what to keep in mind:
</p>

<img src="/static/post_assets/null-and-missing-data-python/null-data-venn-diagram.svg" class="inline_image_full" alt="Data."/>

<h3>How is null data implemented in Python?</h3>

<p>
    <code class="inline_code">R</code> is the only programming language which handles null semantics with what is known as a
    <a href="https://stat.ethz.ch/R-manual/R-devel/library/base/html/NA.html"><strong>true NA</strong></a>. In <code
            class="inline_code">R</code> there is a distinction between <code class="inline_code">null</code> values and
    <code class="inline_code">NA</code> values, allowing <code class="inline_code">R</code> users to easily implement
    the distinction between missing data and non-data with very little mental overhead.
</p>
<p>
    Most every other programming language, Python included, has only a single null datatype: <code
        class="inline_code">None</code> (or <code class="inline_code">float.NaN</code>) in Python. This means that
    Python programs have to virtualize <code class="inline_code">NA</code> ourselves somehow. Generally speaking
    there are two ways of doing this.
</p>

<p>
    The first method is to use a <strong>sentinel value</strong>, a special bit pattern in the data column's native
    type flagging a missing value. This is a traditional way of indicating missing data—recording unknown income as
    <code class="inline_code">-99</code> or an unknown year as <code class="inline_code">0</code>, for example—which is
    used in-place (hopefully with documentation!) by many datasets.
</p>
<p>
    This should seem familiar:
</p>

<img src="/static/post_assets/null-and-missing-data-python/sentinal-value-histogram-example.svg"
      class="inline_image_full" alt="Data."/>

<p>
    The trouble is that sentinal values are in no way durable. <code class="inline_code">-99</code> could be a valid year
    and <code class="inline_code">0</code> could be a valid income! Without a special reserved keyword, a sentinel
    robs you of a value that you might otherwise want to use. They have to be considered on a case-by-case,
    column-by-column basis, a burdensome thing.
</p>
<p>
    The second method is to use a <strong>mask</strong>: a separate boolean array to "mask" the data whenever
    missing data needs to be represented. The advantage here is that since your nullity pattern is encoded (quite
    literally) in a whole second array you don't need to reserve any representations in your core data, meaning that
    this design pattern is more robust:
</p>

<img src="/static/post_assets/null-and-missing-data-python/mask-example.svg" class="inline_image_full" alt="Data."/>

<p>
    The disadvantage is that now you've got a whole second object to tend to—trading complexity and performance for
    durability.
</p>
<p>
    The numerical core of the Python scientific library and the level at which this problem ought to be mediated is
    the <a href="http://www.numpy.org/"><code class="inline_code">numpy</code></a> library, which defines all of the
    efficient linear algebra and matrix manipulation methods necessary for computerized computations. <code
        class="inline_code">numpy</code> provides <code class="inline_code">np.nan</code>, a computationally improved
    version of Python's base <code class="inline_code">None</code>, but it does <em>not</em> provide a true NA
    datatype!
</p>
<p>
    <code class="inline_code">numpy</code> does provide a mask, the <a
        href="http://docs.scipy.org/doc/numpy-1.10.1/reference/maskedarray.generic.html"><code class="inline_code">
    numpy.ma</code></a> module, which implements a masked <code class="inline_code">ndarray</code> subtype called
    <code class="inline_code">MaskedArray</code>. But this implementation is not performant enough, for whatever
    reason, for general purpose use.
</p>
<p>
    This issue has proved to be conscientious. In a document <a
        href="http://www.numpy.org/NA-overview.html">summarizing the great NA debate</a> <code class="inline_code">
    numpy</code> core developers stated that:</p>

<div class="inline_quote">
    <p>After <code class="inline_code">R</code>, <code class="inline_code">numpy.ma</code> is probably the most mature
        source of experience on missing-data-related APIs...while it seems to be generally considered sub-optimal for
        general use, it’s hard to pin down whether this is because the API is immature but basically good, or the API
        is fundamentally broken, or the API is great but the code should be faster, or what. We looked at some of
        those users to try and get a better idea...
    </p>
    <p>...these examples make it clear that there is demand for a convenient way to keep a data array and a mask
        array (or even a floating point array) bundled up together and “aligned”. But they don’t tell us much about
        what semantics the resulting object should have with respect to ufuncs and friends.
    </p>
</div>
<p>
    This reflection hits the nail on the head. By failing to coerce a true NA type as a part of its standard API
    <code class="inline_code">numpy</code> fails to standardize and routinize handling the data it ought to
    represent, forcing users and packages using <code class="inline_code">numpy</code> to track both objects
    themselves instead.
</p>
<p>
    This proves to be such a big burden that the <code class="inline_code">pandas</code> library, the workhorse of
    data-driven Python (built atop <code class="inline_code">numpy</code>) actually <em>eschews</em> using the <code
        class="inline_code">ma</code> mask!
</p>
<p>
    It actually takes a step backwards, coercing all null data into <code class="inline_code">NaN</code> (or <code
        class="inline_code">NaT</code>, a thin time-series wrapper).
</p>
<p>
    The <code class="inline_code">pandas</code> <a href="http://pandas.pydata.org/pandas-docs/stable/missing_data.html">missing data
    tutorial</a> says:</p>

<div class="inline_quote">
    <p>
        The choice of using NaN internally to denote missing data was largely for simplicity and performance
        reasons...we are hopeful that NumPy will soon be able to provide a native NA type solution (similar to R)
        performant enough to be used in pandas.
    </p>
</div>

<p>
    In "Python for Data Analytics" Wes McKinney, the creator of <code class="inline_code">pandas</code>, states
    further:
</p>

<div class="inline_quote">
    <p>
        I do not claim that pandas's NA representation is optimal, but it is simple and reasonably consistent. It's the
        best solution, with good all-around performance characteristics and a simple API, that I could concoct in the
        absence of a true NA data type or bit pattern in NumPy's data types. Ongoing development work in NumPy may
        change this in the future.
    </p>
</div>

<p>
    Nevertheless this situation stands today. Most of the time it is up to you, the developer, to keep track of your
    data nullity pattern yourself. For instance in <a
        href="http://stackoverflow.com/questions/28654325/what-is-pythons-equivalent-of-rs-na">answering a question
    about nullity in Python machine learning</a> a core <code class="inline_code">scikit-learn</code> developer had
    to say:
</p>

<div class="inline_quote">
    <p>
        Scikit-learn doesn't handle missing values currently. For most machine learning algorithms, it is unclear how
        to handle missing values, and so we rely on the user of handling them prior to giving them to the algorithm.
        Numpy doesn't have a "missing" value. Pandas uses NaN, but inside numeric algorithms that might lead to
        confusion. It is possible to use masked arrays, but we don't do that in scikit-learn (yet).
    </p>
</div>

<p>
    Our one null datatype simply cannot stretch to cover both of our use cases, missing data and non-data, creating
    ambiguity that in turn imposes limitations in the input that various packages will accept. This shifts the burden
    of representation onto the programmer, forcing us to keep track of what is data and what isn't and what is valid
    input and what isn't ourselves.
</p>

<p>
    What is the alternative? In an ideal, NA-driven world, Python developers could import the dataset, plug in NA and
    NaN values in once, and then forget about it:
</p>

<img src="/static/post_assets/null-and-missing-data-python/jupyter-notebook-alternative-sceenshot-1.svg" class="inline_image_full"
     alt="Data."/>

<p>
    I wouldn't need to worry about keeping track of or denoting null locations being missing and null streets being
    structural because that fact is immediately obvious from the data. Any modules I ship the data to will know that,
    too, and handle it autonomously and/or expose finer-grained tools&mdash;<code class="inline_code">fillna</code>,
    <code class="inline_code">dropnan</code> and <code class="inline_code">isnull</code>&mdash;for massaging things
    into shape.
</p>

<p>
    But such a far-reaching change would require serious re-thinking of <code class="inline_code">numpy</code> code,
    which—so far—hasn't happened.
</p>

<h3>What are some practical tools for understanding null data?</h3>

<p>
    So far we've examined the theoretical aspects of null data, looking at the difference between non-data and
    missing data and further at the cleavage of missing data into the statistically important subclasses of data missing
    at random and data not missing at random. We then discussed the idea of an NA datatype, and how it might be
    implemented in Python using sentinel values or using a mask. We examined the limitations of <code
        class="inline_code">numpy</code> and the knock-on effect that has on the rest of Python data processing, and
    thought about how things would be different if those limitations weren't there.
</p>

<p>
    Let's close things out by taking a look at a few practical expressions for exploring null data in Python.
</p>
<!--<p>-->
    <!--Note that I am <em>not</em> talking about how to interpolate data. This is covered very extensively in all sorts-->
    <!--of places; a great starting point is <a href="http://pandas.pydata.org/pandas-docs/stable/missing_data.html">the-->
    <!--<code class="inline_code">pandas</code> tutorial on the subject</a>. I am instead focusing on the necessary prior-->
    <!--consideration of understanding what it is, exactly, that you are interpolating.-->
<!--</p>-->
<p>
    <code class="inline_code">pandas</code> nullity hinges the following three critical methods:
</p>
<ul>
    <li>
        The <code class="inline_code">isnull()</code> and <code class="inline_code">notnull()</code> methods
        type-cast to a boolean mask which returns whether the column is null or not.
    </li>
    <li>
        The <code class="inline_code">any()</code> expression can be used to check if <em>any</em> of the columns in
        a <code class="inline_code">pandas DataFrame</code> are filled.
    </li>
    <li>
        The <code class="inline_code">all()</code> expression checks if <em>all</em> entries are filled.
    </li>
</ul>
<p>
    You can chain these to vet your dataset. For instance, <code class="inline_code">df.notnull().all()</code>
    informs you of column completion:
</p>

<img src="/static/post_assets/null-and-missing-data-python/jupyter-screenshot-1.png" class="inline_image"
     style="width:30%;" alt="Data."/>

<p>
    We can count the number of empty slots with <code class="inline_code">df.isnull().sum()</code>:
</p>

<img src="/static/post_assets/null-and-missing-data-python/jupyter-screenshot-2.png" class="inline_image"
     style="width:30%;" alt="Data."/>

<p>
    Attach <code class="inline_code">.plot(kind='bar')</code> to that to get a nice quick plot of the situation:
</p>

<img src="/static/post_assets/null-and-missing-data-python/jupyter-screenshot-3.png" class="inline_image_full"
     alt="Data."/>

<p>
    With just a teeny bit more cleverness you can get this in percentage terms using
    <code class="inline_code">df.isnull().sum() / len(collisions)</code>:
</p>

<img src="/static/post_assets/null-and-missing-data-python/jupyter-screenshot-4.png" class="inline_image"
     style="width:30%;" alt="Data."/>

<p>
    Notice that, again, the unit of transaction in all of this is binary&mdash;data is either defined or not defined.
</p>

<p>
    More advanced tools for leafing through null data are provided by the <a
        href="https://github.com/ResidentMario/missingno"><code class="inline_code">missingno</code></a> library,
    which I wrote expressly for this purpose.
</p>

<p>
    <code class="inline_code">missingno.matrix(df)</code> exposes null values directly, providing a visual summary of
    where the holes are, so to speak:
</p>

<img src="/static/post_assets/null-and-missing-data-python/missingno-matrix.png" class="inline_image_full"
     alt="Data."/>

<p>
    Once I've properly accounted for sentinels, how do I examine whether data is missing at random or not at random?
    One way to do that is to examine pairwise correlations in your variables. The heatmap visualization gives you the
    <code class="inline_code">missingno.heatmap(df)</code> one-liner to do just that:
</p>

<img src="/static/post_assets/null-and-missing-data-python/missingno-heatmap.png" class="inline_image_full"
     alt="Data."/>

<p>
    The dendrogram visualization builds on this by linking up not just pairs but arbitrarily large sets of variables
    which tend to appear alongside one another. Variables which appear alongside one another—if one is defined all
    tend to be defined, if one is null all tend to be null—occur at the ends of the leaves.
</p>

<img src="/static/post_assets/null-and-missing-data-python/missingno-dendrogram.png" class="inline_image_full"
     alt="Data."/>

<p>
    You can read all about it&mdash;including more specifics and capacities of <code class="inline_code">missingno</code>&mdash;<a href="https://github.com/ResidentMario/missingno">on GitHub</a>.
</p>

<p>
    Thoughts? Ideas? More expressions useful for analyzing missing data in Python? Leave a word in the comments!
</p>

{% endblock %}